{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eedf240f-6c0d-4390-908e-b9bac4982229",
   "metadata": {},
   "source": [
    "<h2>Commentary Analysis - All videos<h2>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2195cde5-4144-448c-b2a8-90d51e337147",
   "metadata": {},
   "source": [
    "We now analyse all the videos of the month you selected.\n",
    "\n",
    "First, import some important paths and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9642db4-6033-40ec-beee-4daa6ef45834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To customize depending on the user:\n",
    "path = \"C:\\\\Users\\\\john.doe\\\\Documents\\\\My_Directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bfe374-29d2-4a6d-ad39-66284c530d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Libraries for webscrapping\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "#Libraries for vectorisation and clustering\n",
    "from collections import Counter\n",
    "\n",
    "#Libraries for preprocessing\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "from textblob import TextBlob\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "#nltk.download('punkt')\n",
    "\n",
    "#Libraries for vectorisation and clustering\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Libraries for LDA\n",
    "from gensim import corpora, models\n",
    "\n",
    "#Libraries for visualization\n",
    "import webcolors\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "# Other functions\n",
    "def flatten_list(_2d_list):\n",
    "    flat_list = []\n",
    "    for element in _2d_list:\n",
    "        for item in element:\n",
    "                flat_list.append(item)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628983da-fe6b-41b0-9cec-6b791b810502",
   "metadata": {},
   "source": [
    "<h3>Retrieve the video comments<h3>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d44845d1-a288-4a10-b9fe-d69c3ea4ec5b",
   "metadata": {},
   "source": [
    "We will use Selenium to extract the comments on the videos of January 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed32328-d15d-454d-8eba-8a2a6d242066",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path + 'VideoJanuary2022URLs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c6b090-782d-4c9a-8fab-fbbe1200e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[1:14]\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ac900f-df37-4697-8170-ed53dda1202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Selenium to extract the webpage contents, in particular the comments and associated likes.\n",
    "\n",
    "# One variable will contains all the video comments as one single list of comments : Comments\n",
    "# One variable will contains all the video comments as a list-of-list-of-comments-per-video : CommentsSep\n",
    "\n",
    "Comments = []\n",
    "CommentsSep = []\n",
    "Likes = []\n",
    "for url_ in df['URLs']:\n",
    "    print(url_)\n",
    "    CommentsSep_temp = []\n",
    "    with Chrome() as driver:\n",
    "        wait = WebDriverWait(driver,10)\n",
    "        driver.get(url_)\n",
    "        for item in range(3): \n",
    "            wait.until(EC.visibility_of_element_located((By.TAG_NAME, \"body\"))).send_keys(Keys.END)\n",
    "            time.sleep(3)\n",
    "        for comment in wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"#comment #content-text\"))):\n",
    "            temp = comment.text.strip('\\n')\n",
    "            Comments.append(temp)\n",
    "            CommentsSep_temp.append(temp)\n",
    "        for likes in wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"#vote-count-middle\"))):\n",
    "            Likes.append(likes.text)\n",
    "    CommentsSep.append(CommentsSep_temp)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40900180-5380-4026-b6bf-bafeec8c38bd",
   "metadata": {},
   "source": [
    "<h3>LDA topic extraction visualisation<h3>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77d585bd-f8ea-4f07-8fc0-a95e73b861f6",
   "metadata": {},
   "source": [
    "We build a list of list-of-tokens from all video comments \n",
    "shape needed: [[\"word1\",\"word2\",\"word3\"],[\"word1\",\"word2\"],[\"word1\",\"word2\",\"word3,\"word4\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3287a141-2164-487e-b416-4ce8b3dc49f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CommentsSepTWO = CommentsSep[0:12]\n",
    "list_of_list_of_tokens = []\n",
    "\n",
    "for c_ in range(len(CommentsSepTWO)):\n",
    "    temp = ','.join(CommentsSepTWO[c_])\n",
    "    temp = test.tokenize(temp)\n",
    "    temp = [x.lower() for x in temp]\n",
    "    temp = [lemmatizer.lemmatize(x) for x in temp]\n",
    "    temp = [word for word in temp if word not in fr_stop]\n",
    "    temp = [word for word in temp if word not in SW]\n",
    "    temp = [i for i in temp if not i.isdigit()]\n",
    "    temp = list(filter(lambda x: len(x) > 1, temp))\n",
    "    list_of_list_of_tokens.append(temp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6dbad7d-cbe7-4b45-944f-bc8e1a26c629",
   "metadata": {},
   "source": [
    "We define our LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282297e0-316f-4a92-b8b5-a1c0a1f5c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "\n",
    "dictionary_LDA = corpora.Dictionary(list_of_list_of_tokens)\n",
    "#dictionary_LDA.filter_extremes(no_below=3)\n",
    "corpus = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in list_of_list_of_tokens]\n",
    "\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, \\\n",
    "                            id2word=dictionary_LDA, \\\n",
    "                            passes=4, alpha=[0.01] * num_topics, \\\n",
    "                            eta=[0.01] * len(dictionary_LDA.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16f94d-dfc2-4b9f-8550-45642ac1d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "554dd1b6-26e9-4e94-a7af-b0dce278c07b",
   "metadata": {},
   "source": [
    "Let's visualize the topics. We use the python interactive visualisation library pyLDAvis.\n",
    "\n",
    "Probability mass function frequency of a word given a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83083601-7141-45e1-ae04-312ce4beb7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "from gensim import corpora, models\n",
    "#vis = pyLDAvis.gensim_models.prepare(topic_model=lda_model, corpus=corpus, dictionary=dictionary_LDA)\n",
    "#pyLDAvis.enable_notebook()\n",
    "#pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3be828a-050c-4ff1-a3a0-4933b47d32b0",
   "metadata": {},
   "source": [
    "The visualization allows us to categorize the topics of the videos.\n",
    "\n",
    "You now need to play with the settings to find the some optimized topic extraction.\n",
    "\n",
    "Have fun !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
