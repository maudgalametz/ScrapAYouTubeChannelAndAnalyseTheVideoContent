{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eedf240f-6c0d-4390-908e-b9bac4982229",
   "metadata": {},
   "source": [
    "<h2>Commentary Analysis - On one video<h2>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1259f0e6-6555-4c48-9a6e-b20219560a8e",
   "metadata": {},
   "source": [
    "Now that the video statistics have been saved, we can start analysing the \n",
    "video with the largest number of views.\n",
    "\n",
    "The tools investigated in this jupyter notebook will then be used to analyse all of the selected month videos.\n",
    "\n",
    "First, import the libraries needed to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8958b-1e81-4dc9-8707-d177feb8cdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To customize depending on the user:\n",
    "path = \"C:\\\\Users\\\\john.doe\\\\Documents\\\\My_Directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bfe374-29d2-4a6d-ad39-66284c530d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Libraries for webscrapping\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "#Libraries for vectorisation and clustering\n",
    "from collections import Counter\n",
    "\n",
    "#Libraries for preprocessing\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "from textblob import TextBlob\n",
    "#nltk.download('punkt')\n",
    "\n",
    "#Libraries for vectorisation and clustering\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Libraries for visualization\n",
    "import webcolors\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43487fb-567c-40ff-90a7-290987df371b",
   "metadata": {},
   "source": [
    "<h2>Analysing comments under one video<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628983da-fe6b-41b0-9cec-6b791b810502",
   "metadata": {},
   "source": [
    "<h3>Retrieve the video comments<h3>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d44845d1-a288-4a10-b9fe-d69c3ea4ec5b",
   "metadata": {},
   "source": [
    "We will use Selenium to extract the comments on the most-viewed video of January 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ab4d1b-9499-416e-ad03-df3e04134631",
   "metadata": {},
   "outputs": [],
   "source": [
    "videoId = 'My_video_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ac900f-df37-4697-8170-ed53dda1202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Selenium to extract the webpage contents, in particular the comments and associated likes.\n",
    "\n",
    "with Chrome() as driver:\n",
    "    wait = WebDriverWait(driver,10)\n",
    "    driver.get(\"https://www.youtube.com/watch?v=\" + videoId)\n",
    "\n",
    "    for item in range(3): \n",
    "        wait.until(EC.visibility_of_element_located((By.TAG_NAME, \"body\"))).send_keys(Keys.END)\n",
    "        time.sleep(3)\n",
    "\n",
    "    Comments = []\n",
    "    Likes = []\n",
    "    URLs = []\n",
    "    for comment in wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"#comment #content-text\"))):\n",
    "        Comments.append(comment.text.strip('\\n'))\n",
    "    for likes in wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"#vote-count-middle\"))):\n",
    "        Likes.append(likes.text)\n",
    "        \n",
    "    print(Comments[0:5])\n",
    "    print(Likes[0:5])\n",
    "  \n",
    " #    print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b3e35a-2ef9-45e1-8a6a-0d49727ad978",
   "metadata": {},
   "source": [
    "<h4> Analyse the word cloud of the comments<h4>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "077be8b1-e16b-4b14-a327-896a6ece0b99",
   "metadata": {},
   "source": [
    "Now that the comments are retrieved and stored in the variable 'Comments', we save them in one unique list of elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af9607f-8480-4775-9cb7-f4fbe36eb785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "test = nltk.RegexpTokenizer(r'\\w+')\n",
    "SaveWords = []\n",
    "for i in range(len(Comments)):\n",
    "    SaveWords.append(test.tokenize(Comments[i]))  \n",
    "    \n",
    "def flatten_list(_2d_list):\n",
    "    flat_list = []\n",
    "    for element in _2d_list:\n",
    "        for item in element:\n",
    "                flat_list.append(item)\n",
    "    return flat_list\n",
    "\n",
    "SaveWords = flatten_list(SaveWords)\n",
    "SaveWords[0:10]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d9f2573-12b3-4292-af16-0158d3a19a6f",
   "metadata": {},
   "source": [
    "We first want to count the number of occurences of a given word in the list.\n",
    "\n",
    "We start with lowering the case of all elements, then count the occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba4b36f-4444-4ddc-8633-19834f9d4266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most commonly-used words\n",
    "\n",
    "SaveWords = [x.lower() for x in SaveWords]\n",
    "SaveWords = [str(x) for x in SaveWords]\n",
    "CountWords = Counter(SaveWords)\n",
    "CountWords.most_common()[0:10]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92dfee1c-0ca5-40fe-96fc-3bfe8a838226",
   "metadata": {},
   "source": [
    "Stop words are common words we should avoid including in the list.\n",
    "We use stopwords in NLTK to remove these words from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5366665d-6f60-42d5-bac6-21ecc950b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most commonly-used words - no stop words\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "SaveWords_filtered = [word for word in SaveWords if word not in fr_stop]\n",
    "CountWords_filtered = Counter(SaveWords_filtered)\n",
    "CountWords_filtered.most_common()[0:10]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb78563b-302d-4dc6-8a97-152f4b1d6fee",
   "metadata": {},
   "source": [
    "Let's also suppress words of only one letter, e.g. the verb 'avoir' in its 'a' form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c739ce32-f6df-4065-ace6-83cd6580dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most commonly-used words - last results\n",
    "SaveWords_filtered = list(filter(lambda x: len(x) > 1, SaveWords_filtered))\n",
    "CountWords_filtered = Counter(SaveWords_filtered)\n",
    "CountWords_filtered.most_common()[0:10]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23b148b1-9949-44cb-9a94-201145e44dc6",
   "metadata": {},
   "source": [
    "Add my own customized Stop Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e25df57-f165-4390-a09c-02dc23e2d8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "SW = pd.read_csv(path + 'StopWords_mine.csv', encoding='latin-1')\n",
    "SW = list(itertools.chain.from_iterable([x.replace(\"'\",\"\").split(',') for x in SW[\"StopWords\"]]))\n",
    "SaveWords_filtered = [word for word in SaveWords_filtered if word not in SW]\n",
    "CountWords_filtered = Counter(SaveWords_filtered)\n",
    "CountWords_filtered.most_common()[0:10]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17cc41d4-e53f-4cea-9905-41a4e6bdad69",
   "metadata": {},
   "source": [
    "We build a word cloud to visually see the most commonly used words in the video commentaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23392c8e-4bb4-4a40-8d7f-e2203a5d7ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud\n",
    "\n",
    "wordcloud = WordCloud(background_color = 'white', max_font_size=50).generate(str(SaveWords_filtered))\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)#, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7ceb91-f4c2-401f-83bf-0e7a925ca082",
   "metadata": {},
   "source": [
    "<h3> Comments likes versus sentiment expressed  <h3>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "162de6e1-6cec-4b86-88ae-f297875e52fb",
   "metadata": {},
   "source": [
    "Let's now use the Textblob library to estimate the feeling expressed in each comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d665e58-eb05-4021-9ab3-bceb3b8c9fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textblob\n",
    "#!pip install textblob-fr\n",
    "from textblob import TextBlob\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2e472c-a49b-4e30-98e2-306d2a3224cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring depending on the feeling expressed in the comment\n",
    "Scoring = []\n",
    "for c_ in Comments:\n",
    "    Scoring.append(TextBlob(c_, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer()).sentiment[0])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7794f6c-2c7e-46bb-aa71-5dc3611c82a2",
   "metadata": {},
   "source": [
    "Scoring will be 1 for positive feelings, and close to 0 for negative feelings.\n",
    "\n",
    "We also retrieved the number of likes per commentaries. We can thus study whether positive commentaries are more liked than others. We save Comments, Textblob Scoring and likes in a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e2b6d-ece1-4fc2-8dcd-7a45aac75a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "Likes_int = pd.to_numeric(Likes)\n",
    "d = {'Comments': Comments, 'Scoring': Scoring, 'Likes': Likes_int}\n",
    "Results = pd.DataFrame(data=d)\n",
    "Results = Results.sort_values('Scoring' , ascending=False)\n",
    "Results['Likes'] = Results['Likes'].fillna(0)\n",
    "Results[0:6]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "015f29e4-f3ad-4390-90b4-eecad98dc18c",
   "metadata": {},
   "source": [
    "We now plot the potential correlation between positivity and likes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7437071-9b97-49ba-b445-56c40e73ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Results['Scoring'], Results['Likes'])\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Negativity -> Positivity')\n",
    "plt.ylabel('Number of likes for the comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f13c457-9dd1-4bba-b210-d41486d432b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = np.corrcoef(Results['Scoring'], Results['Likes'])\n",
    "rho"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d164ac7-3e58-4779-be24-d8f341372076",
   "metadata": {},
   "source": [
    "We do not observe a correlation. Conclusion: Positive comments do not attract more likes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065b3c71-8f14-4dc7-a94c-fa45f8eead87",
   "metadata": {},
   "source": [
    "<h3> Topic extraction from comments <h3>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b64ef86c-53d7-4957-98ec-5a1f62dde279",
   "metadata": {},
   "source": [
    "We will attempt a topic extraction using the LDA approach (for Latent Dirichlet Allocation).\n",
    "The dataset we work on will be the full list of the video comentaries \n",
    "\n",
    "We thus are back to the initial 'Comments' variable.\n",
    "\n",
    "Before fitting the LDA model, we have to standardize the words \n",
    "(i.e. remove 's', merge various conjugations° and remove stop words from those comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aa70e3-2d7b-4dcd-8df5-5ec0d249648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemm\n",
    "Comments_lemm = Comments.copy()\n",
    "lemmatizer = FrenchLefffLemmatizer()\n",
    "for c_ in range(len(Comments)):\n",
    "    Comments_lemm[c_] = lemmatizer.lemmatize(str(Comments[c_]))\n",
    "\n",
    "#Stop Words + one-letter word filtering\n",
    "Comments_final = Comments_lemm.copy()\n",
    "for c_ in range(len(Comments)):\n",
    "    temp = test.tokenize(Comments[c_])\n",
    "    temp = [x.lower() for x in temp]\n",
    "    temp = [word for word in temp if word not in fr_stop]\n",
    "    temp = [word for word in temp if word not in SW]\n",
    "    temp = filter(lambda x: len(x) > 1, temp)\n",
    "    Comments_final[c_] =  ' '.join(temp)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80766c7c-7d44-45b0-80e8-7a544e8a976a",
   "metadata": {},
   "source": [
    "We now perform the LDA topic extraction. We assume that the video has two main topics (n_components = 2) and will plot the top 5 words to define the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0441dc04-d7e3-41f2-8c88-bba05192d730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our modelling paramaters\n",
    "n_features = 100\n",
    "n_components = 2 \n",
    "n_top_words = 10\n",
    "\n",
    "# Use Tensorflow to vectorise the comment flow\n",
    "tf_vectorizer = CountVectorizer(analyzer='word', max_features=n_features, max_df=0.95, min_df=2)\n",
    "tf = tf_vectorizer.fit_transform(Comments_final)\n",
    "\n",
    "# Fitting the LDA model \n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=20,\n",
    "    learning_method=\"online\", #using \"batch\" gives the same results, prefer \"online\" for large dataset,\n",
    "    learning_decay=0.7,     #control the learning rate in the \"online\" learning method\n",
    "    learning_offset=50.0,\n",
    "    random_state=0\n",
    ")\n",
    "lda.fit(tf)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10e21fb-7071-404f-b206-e76379b715b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the topics\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2 , figsize=(30, 30), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "plot_top_words(lda, tf_feature_names, n_top_words, \"Topics of the video commentary\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ffb33c3-5a29-47d2-becc-338274666f76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "747394af-41ad-40d7-9213-649f8e38e72b",
   "metadata": {},
   "source": [
    "<h3> Potential improvments<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635af4df-3f0a-472d-b5c8-0ea539c76d03",
   "metadata": {},
   "source": [
    "<h4> Remove non-french comments<h4>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4479dfff-edfc-43d9-91c6-4aa7e43d2bfa",
   "metadata": {},
   "source": [
    "We noticed that some of the comments are in english or other languages (german). \n",
    "The issue with keeping them is that they are going to bias our topic extraction.\n",
    "\n",
    "We can use a n-grams frequency analysis technique to filter non-french comments out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b5421-3568-4307-a3e4-428417b0fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ngramfreq\n",
    "text_categorizer = ngramfreq.NGramBasedTextCategorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3907a78-912d-4c3b-9784-556059600951",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_categorizer.guess_language(\"Brian is in the kitchen.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895bdca5-93f2-446d-8108-783dd4b95c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_categorizer.guess_language(str(Comments[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f4ff1-5db8-4210-97cf-5cfc1e56ea03",
   "metadata": {},
   "source": [
    "<font color='green'>**TBD:** Far from being satisfying for all comments. To be tested.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1772ddb4-a76c-495b-b464-fce3f21ad646",
   "metadata": {},
   "source": [
    "<h4> Use another 'feeling' metric<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da28e403-aa22-4d90-8d9a-fff62e346c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "text          = 'My feelings are good'\n",
    "sent          = TextBlob(text)\n",
    "polarity      = sent.sentiment.polarity\n",
    "current = TextBlob(text, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer()).sentiment[0]\n",
    "print('english comment -->', polarity, 'versus', current)\n",
    "\n",
    "text          = Comments[0]\n",
    "sent          = TextBlob(text)\n",
    "polarity      = sent.sentiment.polarity\n",
    "current = TextBlob(text, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer()).sentiment[0]\n",
    "print('french comment -->', polarity, 'versus', current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e492c-29d4-4b2d-b9bd-ab163a967527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a15d1306-fb8a-44e0-bdae-08ba94f9eff0",
   "metadata": {},
   "source": [
    "<font color='green'>**TBD:** Check if there is a french version to 'polarity'. Another option would be to investigate Stanford Log-Linear Part-Of-Speech Tagger.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2662c9b-8fc9-4f91-a90f-a8ac200d9062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "classifier(Comments[0])\n",
    "classifier('Je ne suis pas du tout d accord')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
